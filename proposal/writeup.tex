\documentclass[11pt]{article}
%SetFonts
% newpxtext+newpxmath
\usepackage{newpxtext}
\usepackage[scaled=.97]{cabin} %ss
\usepackage[varqu,varl]{inconsolata} % tt
\usepackage{amsmath}
\usepackage[bigdelims]{newpxmath}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
%SetFonts


\usepackage{epsfig}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{xspace}

\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\usepackage{enumitem}                     

 
 \usepackage{titlesec}

\titleformat*{\section}{\bfseries}
\titleformat*{\subsection}{\bfseries}
\titleformat*{\subsubsection}{\bfseries}
\titleformat*{\paragraph}{\bfseries}
\titleformat*{\subparagraph}{\bfseries}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document begins here %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}

\noindent {\large {\bf 600.468 Machine Tranlation} \hfill {{\bf Fall 2017}}}\\
{{\bf Project Proposal}} \hfill {{\bf Name:} Yu Zhao, Fan Yang, Zikun Chen} \\
\part{Problem and Background}
While neural machine translation (NMT) is making good progress in recent years, parallel corpuses with millions of sentence pairs of target and source language are urgently needed for training process. However, bilingual data needs tons of human effort to generate and this process is very costly. Meanwhile, monolingual data is much more plentiful so we wonder whether there is some way to use mainly monolingual data with limited bilingual data to achieve comparable performance to traditional NMT with only parallel corpus.


We get our inspiration from one article\cite{he2016dual} we read during preliminary literature review, in this article they introduced a dual learning mechanism which helps NMT systems to automatically learn from unlabeled data in a dual learning game. Besides this dual learning mechanism they proposed, they also reported a very interesting observation: every machine translation task has a dual task, which is very useful in terms of providing feedbacks on how good a language model is. 
\\
\indent We borrowed the idea of "dual learning" and came up with a more specific topic: to investigate language model improvements using only monolingual corpus. We still need some aligned bilingual sentences to build the base models for primal task (French -> English) and dual task (English -> French), but in the subsequent reconstruction closed loop based on dual learning and interactive feedbacks, we can use only monolingual data. And it is our main interest to see how much monolingual corpus can contribute to the accuracy improvements, and we are also interested in how complete our experimental reimplementation of this "dual learning" idea can be, given that the research we based our project on doesn't provide any source code.

\part{Outline}
In encoder-decoder architecture for NMT, we find that actually it could be trained in both direction with parallel sentence pairs, e.g., English to French and French to English. So, it would form a closed loop and two translation models might have a chance to improve themselves given another's output.

To be more specific, let's consider following scenario:
\begin{enumerate}[label*=\arabic*.]
\item The first agent A, who only has a language model of F and a translation model from F to E, receives a sentence in F, translates it to E and forward translated sentence to the second agent.

\item The second agent B, who only has a language model of E and a translation model from E to F, receives the translated sentence in E. Then B checks and notifies the first agent whether it is a natural sentence in E using language model of E. Next, B translates it back to F and forward it to A.

\item Now A checks and notifies B whether the message A receives is consistent with the original. Through the feedback, both A and B will know the performance of their translation model well and can improve them accordingly.

\item This process can be iterated for many rounds until both translation models converge or other criteria like maximum iterations.
\end{enumerate}
Let's denote above model dual NMT. Notice that language model could be obtained using only monolingual data and we might want some parallel data to warm up two translation models.

We're going to use NMT built in HW5 for translation models in our final project and Pytorch to implement the rest, including language models training,  reinforcement learning process between two models and beam search during testing.
\part{Evaluation}
We're going to compare the performance on French $\leftrightarrow$ English translation of standard NMT (one we implemented in HW5) with bilingual data \href{http://www.statmt.org/europarl/}{\color{blue}{Europarl v7}} and dual NMT as described above with monolingual data \href{http://www.statmt.org/wmt17/translation-task.html}{\color{blue}{News Crawl: articles from 2010}}.  Following common practices, we use \href{http://data.statmt.org/wmt16/translation-task/dev.tgz}{\color{blue}{news-test2012}} and \href{http://data.statmt.org/wmt16/translation-task/dev.tgz}{\color{blue}{news-test2013}} as the validation and test data respectively.  BLEU score will be used as the evaluation metric for two models.
\part{Reading List}
\begingroup
\renewcommand{\section}[2]{}
\bibliography{./final}
\bibliographystyle{ieeetr}
\nocite{*}
\endgroup
\end{document}