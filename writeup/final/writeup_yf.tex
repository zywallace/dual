\documentclass[11pt]{article}
%SetFonts
% newpxtext+newpxmath
\usepackage{newpxtext}
\usepackage[scaled=.97]{cabin} %ss
\usepackage[varqu,varl]{inconsolata} % tt
\usepackage{amsmath}
\usepackage[bigdelims]{newpxmath}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
%SetFonts

% For algorithms
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{amssymb}

\usepackage{epsfig}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{indentfirst}

\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\usepackage{enumitem}


 \usepackage{titlesec}

\titleformat*{\section}{\bfseries}
\titleformat*{\subsection}{\bfseries}
\titleformat*{\subsubsection}{\bfseries}
\titleformat*{\paragraph}{\bfseries}
\titleformat*{\subparagraph}{\bfseries}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document begins here %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}

\noindent {\large {\bf 600.468 Machine Tranlation} \hfill {{\bf Fall 2017}}}\\
{{\bf Final Report} \hfill {{\bf Name:} Yu Zhao, Fan Yang, Zikun Chen} \\
\part{Problem and Background}
As neural machine translation (NMT) is making good progress in recent years, parallel corpuses with millions of sentence pairs of target and source language are urgently needed for training process. However, bilingual data needs tons of human effort to generate and this process is very costly. Meanwhile, monolingual data is much more plentiful and accessible compared to bilingual corpus. Hence, we wonder whether there is some way to use mainly monolingual data with limited amount of bilingual data to achieve comparable performance to traditional NMT with only parallel corpus.
\newline

We get our inspiration from one article\cite{he2016dual} we read during preliminary literature review, in this article they introduced a dual learning mechanism which helps NMT systems to automatically learn from unlabeled data in a dual learning game. Besides this dual learning mechanism they proposed, they also reported a very interesting observation: every machine translation task has a dual task, which is very useful in terms of providing feedbacks on how good a language model is.
\\
\newline
\indent We borrowed the idea of "dual learning" and came up with a more specific topic: to investigate language model improvements using only monolingual corpus. We still need some aligned bilingual sentences to build the base models for primal task (French $\to$ English) and dual task (English $\to$ French), but in the subsequent reconstruction closed loop based on dual learning and interactive feedbacks, we can use only monolingual data. And it is our main interest to see how much monolingual corpus can contribute to the accuracy improvements, and we are also interested in how complete our experimental reimplementation of this "dual learning" idea can be, given that the research we based our project on doesn't provide any source code.

\part{Outline}
In an encoder-decoder architecture for NMT, it is believed that it can be trained in both direction with parallel sentence pairs, e.g., English to French and French to English. Therefore, this training process would form a repeated closed loop and two translation models should be able to improve themselves interactively given each other's output and feedbacks.
\\
\newline
\indent To be more specific, let's consider the following scenario:
\begin{enumerate}[label*=\arabic*.]
\item The first agent A, who only has a language model of F and a translation model from F to E, receives a sentence in F, translates it to E and forward translated sentence to the second agent.

\item The second agent B, who only has a language model of E and a translation model from E to F, receives the translated sentence in E. Then B checks and notifies the first agent whether it is a natural sentence in E using language model of E. Next, B translates it back to F and forward it to A.

\item Now A checks and notifies B whether the message A receives is consistent with the original. Through the feedback, both A and B will know the performance of their translation model well and can improve them accordingly.

\item This process can be iterated for many rounds until both translation models converge or other criteria like maximum iterations.
\end{enumerate}
\indent \indent  Let's denote above model as dual NMT. Notice that language model could be obtained using only monolingual data and we still need some parallel data to warm up / pre-implement two translation models in advance according to the original paper\cite{he2016dual}.

Note that baseline model in each direction should be trained separately while dual model can train both translation models at the same time.

\part{Algorithm}
Assume we have fixed language models and monolingual corpuses, above scenario could be described as algorithm given below. Notice here reward is log probability so it's negative and greater is better, while the loss function here is negative log probability so we want it to be small.
\begin{algorithm}
\begin{algorithmic}[1]
\State \textbf{Input}: Monolingual corpuses $D_A$ and $D_B$, initial translation models $\Theta_{AB}$ and $\Theta_{BA}$, fixed language models $LM_A$ and $LM_B$, hyper-parameter $\alpha$, $K$, learning rates $\gamma_1,\gamma_2$ .
\Repeat
\State Sample sentence $s_A$ and $s_B$ from $D_A$ and $ D_B$ respectively.

\State Set $s = s_B$
\State Generate top $K$ sentences using beam search according to translation model $\Theta_{AB}$
\For{each intermediate sentence $s_k$}
\State Set the language-model reward for the $k$th sampled sentence as $r_{1,k}=LM_B(s_{k})$.
\State Set the communication reward for the $k$th sampled sentence as $r_{2,k}=\log P(s|s_{k};\Theta_{BA})$.
\State Set the total reward of the $k$th sample as $r_k = \alpha r_{1,k} + (1-\alpha)r_{2,k}$.
\EndFor
\State Update Model:
\begin{align*}
\nabla_{\Theta_{AB}} E[r] &= \frac{1}{K}\sum^K_{k=1}[r_k\nabla_{\Theta_{AB}}\log P(s_{k}|s; \Theta_{AB})]&\Theta_{AB} \leftarrow\Theta_{AB} + \gamma_1\nabla_{\Theta_{AB}} E[r]\\
\nabla_{\Theta_{BA}} E[r] &= \frac{1}{K}\sum^K_{k=1} [(\alpha - 1)\nabla_{\Theta_{BA}} \log P(s|s_{k};\Theta_{BA})]&\Theta_{BA} \leftarrow\Theta_{BA} + \gamma_2\nabla_{\Theta_{BA}} E[r]
\end{align*}
\State Set $s = s_B$
\State Go through 5 to 11 symmetrically.
\Until{convergence}
\end{algorithmic}
\end{algorithm}

\part{Settings}
We compared the performance on French $\leftrightarrow$ English translation and use OpenNMT-py for all training and evaluation process. Baseline model we compare with our model is conventional NMT model as mentioned below.
\\
\newline
\indent LSTM networks was used and experimental parameters was set according to \cite{bahdanau2014neural}. For each language, the vocabulary consisted of the most common 30,000 words in the parallel corpuses, and unknown words were replaced with a special token as usual.  Sentences with more than 50 words was discarded. We used embedding of size 620 and hidden size of RNN was 1000. Bi-directional RNN was used for encoder. Batch size was 80. Optimizer of AdaDelta was used with learning rate 1. For the simplicity of our model, number of layers of both encoder and decoder was set to be 1. Dropout was set to be 0.3 in training of baseline model and warm up of dual model.
\\
\newline
\indent Our model needs a language model for each language. We used Faster RNNLM (HS/NCE) toolkit \footnote{https://github.com/yandex/faster-rnnlm}\cite{mikolov2010recurrent} for each language to generate language model using its corresponding monolingual corpus. Language model was set as indicated in the paper: GRU and hidden size of 128. For monolingual corpuses, sentences containing unknown tokens was removed. Language models were fixed during the training process of dual model.
\\
\newline
\indent For warmup (in this way our translation model would not just produce garbled sentences), we trained our model (on the same setting as above) on small set of bilingual data. For communication phase, $K=2$ and $\alpha = 0.005$. Optimizer of SGD would be used and learning rate $\gamma_1 = 0.0002, \gamma_2=0.02$.
\\
\newline
\indent For testing, beam search of beam size 12 is used for both baseline model and our model.
\\
\newline
\indent We used corpuses from WMT 14. Baseline NMT was trained on bilingual data News Commentary and dual NMT as described above on monolingual data News Crawl: articles from 2010 and 10\% of News Commentary. Note that we want our training process of dual model to be finished in reasonable time. So we randomly picked 160k sentences (which is almost the same size as bilingual data) from both corpuses when the whole corpuses were still used to train our language models. Following common practices, we use news-test2012 and news-test2013 as the validation and test data respectively.
\\
\newline
\indent Moses tokenizer\footnote{https://github.com/moses-smt/mosesdecoder/tree/master/scripts} were used to preprocess raw data and \texttt{multi-bleu.perl} was used to compute BLEU score.

\part{Results}
The table \ref{table:1} shows performance of baseline and dual model on testing data. 
\begin{table}
\begin{center}
 \begin{tabular}{||c c c||} 
 \hline
  & en $\to$ fr & fr $\to$ en \\ [0.5ex] 
 \hline\hline
 baseline & 15.99 & 15.18 \\
  \hline
 dual & 15.85 & 15.16 \\
  \hline
\end{tabular}
 \caption{BLEU scores on testing data}
\label{table:1}
\end{center}
\end{table}

In original paper\cite{he2016dual}, it was shown dual model outperformed standard NMT(baseline model in our experiment) by around 3 points in both direction. Our implementation couldn't meet the same level of improvement and potential causes will be discussed in the next section.
\part{Discussion}
\section{Overall performance}
Although we didn't produce similar level of improvement from baseline model as in the paper, we still could see our dual model managed to 

\section{Warm up process}
There is a soft-landing strategy mentioned in paper we didn't implement. They tried to smooth the transition from initial model to dual training process by mixing monolingual and bilingual data in batch at the beginning and gradually decreasing the percentage of bilingual data to zero. While we just used monolingual data from start, this could potentially update gradient in the wrong direction.
\section{Parameter choice and Data}
Original paper didn't specify several settings of dual model like the number of layers, if dropout was used and setting of language model, which may change the behavior of our model profoundly.

Since our resource was limited, our couldn't train our model on the same dataset and in the same time as used in the paper. Our bilingual data was about 160 thousand sentences while their was 12 million. Also they didn't mentioned choice of their tokenizer and different tokenizers would produce inconsistent results as mentioned in \texttt{multi-bleu.perl}.
\part{Reference}
\begingroup
\renewcommand{\section}[2]{}
\bibliography{./final}
\bibliographystyle{ieeetr}
\nocite{*}
\endgroup
\end{document}
